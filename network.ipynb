{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b14d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/piotrbaranowski/.venvs/jupyter/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-11-04 00:35:11 URL:https://s3.amazonaws.com/img-datasets/mnist.npz [11490434/11490434] -> \"mnist.npz\" [1]\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm pandas\n",
    "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c7e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from numpy.typing import NDArray\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "FloatNDArray = NDArray[np.float64]\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df2dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(\n",
    "    path: Path = Path(\"mnist.npz\")\n",
    ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
    "\n",
    "    Returns tuple of:\n",
    "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
    "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
    "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
    "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
    "\n",
    "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
    "    \"\"\"\n",
    "    with np.load(path) as f:\n",
    "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "\n",
    "    H = W = 28\n",
    "    N_train = len(x_train)\n",
    "    N_test = len(x_test)\n",
    "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
    "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
    "\n",
    "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
    "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
    "\n",
    "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
    "    y_train[np.arange(N_train), _y_train] = 1\n",
    "\n",
    "    y_test = np.zeros((N_test, 10))\n",
    "    y_test[np.arange(N_test), _y_test] = 1\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f384f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c257b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
    "    m = np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x - m)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "### TESTS ###\n",
    "def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n",
    "    r = stable_softmax(x)\n",
    "    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n",
    "    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n",
    "    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n",
    "\n",
    "def test_stable_softmax() -> None:\n",
    "    x1 = np.random.rand(100, 32).astype(np.float64)\n",
    "    _test_one(x1, unstable_softmax(x1))\n",
    "\n",
    "    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n",
    "    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n",
    "\n",
    "    print(\"OK\")\n",
    "\n",
    "test_stable_softmax()\n",
    "### TESTS END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95d41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResults:\n",
    "    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Map from model name to map from lr to list of test accuracies.\n",
    "        self.results = dict[str, dict[float, list[float]]]()\n",
    "\n",
    "    def clear(self, model_name: str | None = None) -> None:\n",
    "        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n",
    "        if model_name:\n",
    "            if model_name in self.results:\n",
    "                del self.results[model_name]\n",
    "        else:\n",
    "            self.results = {}\n",
    "\n",
    "    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n",
    "        if model_name not in self.results:\n",
    "            self.results[model_name] = {}\n",
    "        if learning_rate not in self.results[model_name]:\n",
    "            self.results[model_name][learning_rate] = []\n",
    "        self.results[model_name][learning_rate].append(accuracy)\n",
    "\n",
    "    def display_results(self) -> None:\n",
    "        data = list[dict[str, Any]]()\n",
    "        for model_name, model_results in self.results.items():\n",
    "            for lr, accuracies in model_results.items():\n",
    "                mean_accuracy = np.mean(accuracies)\n",
    "                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n",
    "                data.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"lr\": lr,\n",
    "                    \"mean_accuracy\": mean_accuracy,\n",
    "                    \"accuracy\": accuracy_summary\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n",
    "        del df[\"mean_accuracy\"]\n",
    "        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_constructor: Callable[[Sequence[int]], Any],\n",
    "        layers: Sequence[int] = (784, 30, 10),\n",
    "        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n",
    "        n_trainings: int = 3,\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        # Automatic model name with parameters.\n",
    "        if kwargs:\n",
    "            if tuple(layers) != (784, 30, 10):\n",
    "                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n",
    "\n",
    "            model_name += \"(\"\n",
    "            for k, v in kwargs.items():\n",
    "                if isinstance(v, (float,  np.floating)):\n",
    "                    model_name += f\"{k}={v:.1g},\"\n",
    "                else:\n",
    "                    model_name += f\"{k}={v},\"\n",
    "            model_name = model_name[:-1]\n",
    "            model_name += \")\"\n",
    "\n",
    "        # Train for each learning rate, n_trainings times.\n",
    "        for lr in learning_rates:\n",
    "            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n",
    "            for i in range(n_trainings):\n",
    "                network = model_constructor(layers, **kwargs)\n",
    "                accuracy = network.train(\n",
    "                    (x_train, y_train),\n",
    "                    epochs=10,\n",
    "                    mini_batch_size=100,\n",
    "                    learning_rate=lr,\n",
    "                    test_data=(x_test, y_test),\n",
    "                )\n",
    "                self.add_result(model_name, lr, float(accuracy))\n",
    "\n",
    "\n",
    "model_results = ModelResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a9e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 3 random trainings with with lr = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  5.01it/s, Test accuracy: 90.76 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.61it/s, Test accuracy: 91.06 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s, Test accuracy: 91.09 %]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 3 random trainings with with lr = 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.53it/s, Test accuracy: 94.73 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.68it/s, Test accuracy: 94.72 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.78it/s, Test accuracy: 94.84 %]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 3 random trainings with with lr = 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  7.01it/s, Test accuracy: 8.92 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  6.72it/s, Test accuracy: 10.09 %]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  5.78it/s, Test accuracy: 8.92 %]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cd62b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_cd62b_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
       "      <th id=\"T_cd62b_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
       "      <th id=\"T_cd62b_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_cd62b_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
       "      <td id=\"T_cd62b_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
       "      <td id=\"T_cd62b_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.1 p.p.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cd62b_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
       "      <td id=\"T_cd62b_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_cd62b_row1_col2\" class=\"data row1 col2\" >91.0% ± 0.1 p.p.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cd62b_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
       "      <td id=\"T_cd62b_row2_col1\" class=\"data row2 col1\" >1e+02</td>\n",
       "      <td id=\"T_cd62b_row2_col2\" class=\"data row2 col2\" >9.3% ± 0.6 p.p.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x146466f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
    "          These are lengths of activation vectors, where:\n",
    "          - N^0 is input size: H * W = 28 * 28 = 784.\n",
    "          - N^last is the number of classes into which we can classify each input: 10.\n",
    "        \"\"\"\n",
    "        self.sizes = list(sizes)\n",
    "\n",
    "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
    "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
    "\n",
    "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
    "        # Weights are indexed by target node first.\n",
    "        self.weights = [\n",
    "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
    "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
    "        ]\n",
    "\n",
    "        self.num_layers = len(self.weights)   # = len(sizes) - 1\n",
    "\n",
    "\n",
    "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
    "        \"\"\"\n",
    "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
    "\n",
    "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
    "        \"\"\"\n",
    "        g = x\n",
    "        for w, b in zip(self.weights, self.biases, strict=True):\n",
    "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n",
    "            g = sigmoid(g @ w.T + b)\n",
    "        return g\n",
    "\n",
    "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
    "\n",
    "        Args:\n",
    "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
    "        - y_mini_batch: shape (B, N^last).\n",
    "        - learning_rate.\n",
    "        \"\"\"\n",
    "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
    "\n",
    "        # Gradient descent step.\n",
    "        self.weights = [\n",
    "            w - learning_rate * grad_w\n",
    "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - learning_rate * grad_b\n",
    "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
    "        ]\n",
    "\n",
    "    def backprop(\n",
    "        self, x: FloatNDArray, y: FloatNDArray\n",
    "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
    "        \"\"\"\n",
    "        Backpropagation for a mini-batch (vectorized).\n",
    "\n",
    "        Args:\n",
    "        - x: input, shape (B, N^0)\n",
    "        - y: target label (one-hot encoded), shape (B, N^last)\n",
    "\n",
    "        Returns (grads_w, grads_b), where:\n",
    "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
    "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
    "        \"\"\"\n",
    "        B, N0 = x.shape\n",
    "        assert N0 == self.sizes[0]\n",
    "\n",
    "        g = x\n",
    "        gs: list[FloatNDArray] = [x]\n",
    "        for w, b in zip(self.weights, self.biases, strict=True):\n",
    "            f = g @ w.T + b\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "\n",
    "        # Backward pass.\n",
    "        grad_g = self.cost_derivative(gs[-1], y)  # Shape initially (B, N^last), then layer by layer.\n",
    "\n",
    "        grads_w = []  # shapes (N^last, N^{last-1}), ..., (N^1, N^0)\n",
    "        grads_b = []  # shapes (N^last,), ..., (N^1,)\n",
    "\n",
    "        for w, prev_g, g in reversed(list(zip(self.weights, gs[:-1], gs[1:], strict=True))):\n",
    "            grad_f = grad_g * g * (1 - g)  # Shape initially (B, N^last), then layer by layer.\n",
    "            grads_w.append(grad_f.T @ prev_g)  # Shape (N^i, B) @ (B, N^{i-1})).\n",
    "            grads_b.append(np.sum(grad_f, axis=0))  # Shape (N^i).\n",
    "            grad_g = grad_f @ w  # Shape (B, N^i) @ (N^i, N^{i-1}).\n",
    "\n",
    "        grads_w.reverse()  # Now shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n",
    "        grads_b.reverse()  # Now shapes (N^1,) ..., (N^last,).\n",
    "\n",
    "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
    "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
    "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
    "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
    "\n",
    "        return grads_w, grads_b\n",
    "\n",
    "\n",
    "\n",
    "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
    "        \"\"\"\n",
    "        Gradient of loss (MSE) over output activations.\n",
    "\n",
    "        Args:\n",
    "        - a: output activations, shape (B, N^last).\n",
    "        - y: target values (one-hot encoded labels), shape (B, N^last).\n",
    "\n",
    "        Returns gradients, shape (B, N^last).\n",
    "        \"\"\"\n",
    "        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
    "        B, N_last = a.shape\n",
    "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
    "\n",
    "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
    "        \"\"\"\n",
    "        Compute accuracy: the ratio of correct answers for test_data.\n",
    "\n",
    "        Args:\n",
    "        - x_test_data: shape (B, N^0).\n",
    "        - y_test_data: shape (B, N^last).\n",
    "        \"\"\"\n",
    "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
    "        targets = np.argmax(y_test_data, axis=1)\n",
    "        return np.mean(predictions == targets)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
    "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
    "        epochs: int = 2,\n",
    "        mini_batch_size: int = 100,\n",
    "        learning_rate: float = 1.0\n",
    "    ) -> np.float64:\n",
    "        x_train, y_train = training_data\n",
    "        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
    "        for epoch in progress_bar:\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                i_begin = i * mini_batch_size\n",
    "                i_end = (i + 1) * mini_batch_size\n",
    "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
    "            if test_data:\n",
    "                x_test, y_test = test_data\n",
    "                accuracy = self.evaluate(x_test, y_test)\n",
    "                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n",
    "\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "            return self.evaluate(x_test, y_test)\n",
    "        else:\n",
    "            return np.float64(-1)\n",
    "\n",
    "model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n",
    "model_results.display_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
